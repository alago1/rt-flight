{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stitching Tutorial\n",
    "\n",
    "The Workflow of the Stitching Pipeline can be seen in the following. Note that the image comes from the [OpenCV Documentation](https://docs.opencv.org/3.4/d1/d46/group__stitching.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following block, we allow displaying resulting images within the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following block, we load the correct img paths to the used image sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def get_image_paths(img_set):\n",
    "    return [str(path.relative_to('.')) for path in Path('../../data/sequential_training').rglob(f'{img_set}*')]\n",
    "\n",
    "sequential_imgs = get_image_paths('')\n",
    "sequential_imgs = sorted(sequential_imgs)[32:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequential_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize Images\n",
    "\n",
    "The first step is to resize the images to medium (and later to low) resolution. The class which can be used is the `ImageHandler` class. If the images should not be stitched on full resolution, this can be achieved by setting the `final_megapix` parameter to a number above 0. \n",
    "\n",
    "`ImageHandler(medium_megapix=0.6, low_megapix=0.1, final_megapix=-1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stitching.image_handler import ImageHandler\n",
    "\n",
    "img_handler = ImageHandler()\n",
    "img_handler.set_img_names(sequential_imgs)\n",
    "\n",
    "medium_imgs = list(img_handler.resize_to_medium_resolution())\n",
    "final_imgs = list(img_handler.resize_to_final_resolution())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Everytime `list()` is called in this notebook means that the function returns a generator (generators improve the overall stitching performance). To get all elements at once we use `list(generator_object)`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_size = img_handler.get_image_size(final_imgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Features\n",
    "\n",
    "On the medium images, we now want to find features that can describe conspicuous elements within the images which might be found in other images as well. The class which can be used is the `FeatureDetector` class.\n",
    "\n",
    "`FeatureDetector(detector='orb', nfeatures=500)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stitching.feature_detector import FeatureDetector\n",
    "\n",
    "finder = FeatureDetector()\n",
    "features = [finder.detect_features(img) for img in medium_imgs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Features\n",
    "\n",
    "Now we can match the features of the pairwise images. The class which can be used is the FeatureMatcher class.\n",
    "\n",
    "`FeatureMatcher(matcher_type='homography', range_width=-1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stitching.feature_matcher import FeatureMatcher\n",
    "\n",
    "matcher = FeatureMatcher()\n",
    "matches = matcher.match_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that:\n",
    "\n",
    "- image 1 has a high matching confidence with image 2 and low confidences with image 3 and 4\n",
    "- image 2 has a high matching confidence with image 1 and image 3 and low confidences with image 4\n",
    "- image 3 has a high matching confidence with image 2 and low confidences with image 1 and 4\n",
    "- image 4 has low matching confidences with image 1, 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a `confidence_threshold`, which is introduced in detail in the next step, we can plot the relevant matches with the inliers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Estimation, Adjustion and Correction\n",
    "\n",
    "With the features and matches we now want to calibrate cameras which can be used to warp the images so they can be composed correctly. The classes which can be used are `CameraEstimator`, `CameraAdjuster` and `WaveCorrector`:\n",
    "\n",
    "```\n",
    "CameraEstimator(estimator='homography')\n",
    "CameraAdjuster(adjuster='ray', refinement_mask='xxxxx')\n",
    "WaveCorrector(wave_correct_kind='horiz')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stitching.camera_estimator import CameraEstimator\n",
    "from stitching.camera_adjuster import CameraAdjuster\n",
    "from stitching.camera_wave_corrector import WaveCorrector\n",
    "\n",
    "camera_estimator = CameraEstimator()\n",
    "camera_adjuster = CameraAdjuster()\n",
    "wave_corrector = WaveCorrector()\n",
    "\n",
    "cameras = camera_estimator.estimate(features, matches)\n",
    "cameras = camera_adjuster.adjust(features, matches, cameras)\n",
    "cameras = wave_corrector.correct(cameras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warp Images\n",
    "\n",
    "With the obtained cameras we now want to warp the images itself into the final plane. The class which can be used is the `Warper` class:\n",
    "\n",
    "`Warper(warper_type='spherical', scale=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stitching.warper import Warper\n",
    "\n",
    "warper = Warper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we set the the medium focal length of the cameras as scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "warper.set_scale(cameras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warp final resolution images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sizes = img_handler.get_final_img_sizes()\n",
    "camera_aspect = img_handler.get_medium_to_final_ratio()    # since cameras were obtained on medium imgs\n",
    "\n",
    "warped_final_imgs = list(warper.warp_images(final_imgs, cameras, camera_aspect))\n",
    "warped_final_masks = list(warper.create_and_warp_masks(final_sizes, cameras, camera_aspect))\n",
    "final_corners, final_sizes = warper.warp_rois(final_sizes, cameras, camera_aspect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excursion: Timelapser\n",
    "\n",
    "The Timelapser functionality is a nice way to grasp how the images are warped into a final plane. The class which can be used is the `Timelapser` class:\n",
    "\n",
    "`Timelapser(timelapse='no')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(112, 224), (224, 112), (224, 224), (112, 336), (112, 448), (224, 336), (224, 448), (112, 560)]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "from stitching.timelapser import Timelapser\n",
    "\n",
    "def get_first_nonzero_pixel(ndarr):\n",
    "    return np.argwhere(ndarr > 0)[0][:2]\n",
    "\n",
    "def get_center(corner, size):\n",
    "    return (round(corner[0] + size[1] / 2), round(corner[1] + size[0] / 2))\n",
    "\n",
    "timelapser = Timelapser('as_is')\n",
    "timelapser.initialize(final_corners, final_sizes)\n",
    "dex = 1\n",
    "centers = []\n",
    "for img, corner, size in zip(warped_final_imgs, final_corners, final_sizes):\n",
    "    timelapser.process_frame(img, corner)\n",
    "    frame = timelapser.get_frame()\n",
    "    centers.append(get_center(get_first_nonzero_pixel(frame), size))\n",
    "\n",
    "print(centers)\n",
    "print(len(centers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "8abf9bbe28e4c81cd50df63cd7889755b2a3ffe62ee65943b7562c9a5a92ef47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
